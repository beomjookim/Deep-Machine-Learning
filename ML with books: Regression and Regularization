21.03.31, 04.01   [p112 ~ p173]
Chapter 03. Regression and Regularization 회귀 알고리즘과 모델 규제

오늘 진행한 프로그램:
https://colab.research.google.com/drive/1DWbzOHHMGl625IXIUkzvT4_RIC889MVW


● 챕터 1, 2 점검

1940년대부터 발전하기 시작한 인공지능은, 1970년대와 1990년에 두 차례의 겨울을 맞았다. 그 외의 기간 동안에는 대부분 문제없이 발전을 해왔고, 2차 겨울 후에는 꾸준히 발전 중.
오픈소스와 클라우드 시스템의 영향으로 전 세계적인 협동 개발 및 발전에 힘입어 머신러닝, 딥러닝, 그 외에 AI 관련 분야까지 유래없는 성장을 진행 중이다.
슬슬 인간의 삶에 영향을 미치기 시작하면서, 일반인들도 AI의 기술적 배경에 관심을 둔다. 이러한 상황에서 소프트웨어 엔지니어들에게 AI 역량을 기대하는 건 어찌보면 당연한 일이다.

AI > ML > DL: 머신 러닝은 컴퓨터가 자동으로 데이터에서 규칙을 학습하게끔 하는 분야. 딥러닝은 머신 러닝 중에서 인공신경망을 기반으로한 방법들.

마켓 예제에서 다룬 중요한 것들:
마켓 예제: 도미와 빙어 수십마리의 데이터를 한 데 섞어서 컴퓨터에게 학습시킨 후, 임의의 데이터를 컴퓨터에게 주고 빙어인지 도미인지 식별할 수 있게 하는 머신러닝 프로그램.
-> KNN 알고리즘을 사용했다. 수많은 데이터들을 컴퓨터에게 주고, 샘플을 넣으면 거리상 가장 가까운 데이터들을 확인, 해당 모집단 중 가장 많은 녀석으로 배정하는 알고리즘.

1. bream_length, bream_weight, smelt_length, smelt_weight 의 데이터를 가지고 시작한다. 
2. 우선 시각화하기 위해서, matplotlib 모듈의 pyplot 모듈을 사용한다. 그 안의 scatter, xlabel, ylabel, show의 함수들을 사용.
3. scikit learn 머신러닝 라이브러리를 활용한다. 사이킷런은 인풋으로 2차원 리스트를 요구하기 때문에 우리의 데이터를 그렇게 바꿔줌.
    (1) 기존의 리스트를 사용하여
        fish_data = [[l,w] for l,w in zip(length, weight)]                  <-- [x for x in zip(k)], k라는 리스트 내부의 x라는 원소들로 리스트화 시키는 좋은 구문과 zip함수.
        fish_target = [1] * 35 + [0] * 14                                   <-- 타겟 값까지 확인.
    (2) 넘파이를 사용하면
        fish_data = numpy.column_stack((fish_length, fish_weight))          <-- 넘파이의 함수 중 하나인 column_stack 함수로 바로 줄세우기
        fish_target = numpy.concatenate((numpy.ones(35), numpy.zeros(14)))  <-- 1과 0을 개수만큼!      
4. scikit learn 패키지와, 그 안의 모듈 중에 하나인 KNeighborsClassifier 모듈을 import 하고 객체를 생성한다 (KNN 알고리즘을 사용하기 위해서).
    from sklearn.neighbors import KNeighborsClassifier
    k = KNeighborsClassifier()
5. 기존의 데이터 세트를 훈련 세트와 테스트 세트로 나눠준다.
    from sklearn.model_selection import train_test_split
    train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify = fish_target, random_state = 42)
        <-- 예제의 특성상 계속 같은 난수가 나와야 하므로 random_state을 지정해줬고, 샘플링 편향을 막기 위해 stratify 변수를 지정해줬다.
6. 이제 훈련과 테스트를 하면 되는데 그 전에, 지금의 x, y 축이 각각 단위가 다른데 KNN 알고리즘은 이 영향을 받으므로 단위를 맞춰주는 스케일링을 해줘야 한다. 표준편차를 이용.
   중요한 점은 훈련 세트와 테스트 세트 모두 다 스케일링을 해줘야 한다는 것이다.
    mean = numpy.mean(train_input, axis = 0)
    std = numpy.std(train_input, axis = 0)
    train_scaled = (train_input - mean)/std
7. 훈련 세트로 훈련을 시키고(KNN의 경우 훈련보다는 저장이 맞는 표현이지만)
    k.fit(train_scaled, train_target)
8. 테스트 세트로 확인을 하면 끝.
    k.score(test_scaled, test_target)

만약 새로운 생선 한 마리 new를 테스트해보고 싶다고 하면 k.predict([new]) 해주면 되는데, 중요한 것은 이 new 값은 역시나 scaled value여야 한다는 것이다.
그리고 스킵했지만 random 난수 받는 등의 과정이 있다.


● KNN 회귀

* 회귀: 전에 다뤘던 것은 샘플을 몇 개의 클래스 중 하나로 분류하는 '분류'였다면, 이번에는 임의의 어떤 값을 예측하는 알고리즘 즉, '회귀'.

농어를 무게 단위로 판매하려 함. 농어의 다른 측량값들은 있는데 무게를 예측해야 하는 상황. 고르는 게 아니라 예측이 필요.
현재 가지고 있는 것: 무게를 정확하게 측정한 샘플 56마리에 대한 데이터.
다행히도 이전에 썼던 KNN 알고리즘을 분류 뿐 아니라 회귀에도 사용할 수 있다는 소식.
기존에는 근처의 데이터 중 많은 클래스의 클래스로 분류를 했다면 이번에는 근처의 데이터들의 평균값을 내어 반환함.

- 데이터 준비
농어의 길이를 특성으로 하고 무게를 타깃으로.

- 결정 계수 Coefficient of Determination (R^2):
사이킷런의 KNN 회귀 클래스는 KNeighborsRegressor임. 이전에 썼던 KNN 분류 클래스인 KNeighborsClassifier와 매우 비슷. 객체를 생성하고 fit() 메서드로 회귀 모델을 훈련.
score를 출력. 결과값이 1에 가깝긴 한데, 이 점수가 바로 결정계수. 계산 방식은 간단함. 1 - ((타깃-예측)2 의 합 / (타깃-평균)2 의 합)

그러나 0.99가 얼마나 큰 수인지에 대한 기준이 없으므로 객관적인 수를 확인하기 위해서 mean_absolute_error를 사용.
mae = mean_absolute_error(test_weight, test_prediction) 이 함수를 통해서 test_prediction 값과 test_weight 값을 비교해서 절대적인 차이가 얼마인지를 확인.
출력값이 19.157 이라고 나온다. 즉 19.157g 정도 다르다는 것을 알 수 있음.

- 과대적합 overfitting vs. 과소적합 underfitting
훈련한 모델을 사용해 훈련 세트의 R^2 점수를 확인한다.
모델을 훈련 세트에 훈련하면 훈련 세트에 맞는 모델이 만들어진다. 이 모델을 훈련 세트와 테스트 세트에서 평가하면 당연히 훈련 세트의 정확도가 더 높아야한다는 것이 직관적 판단.
그래서 일반적으로 훈련 세트의 점수가 테스트 세트의 점수를 어느정도 상회하는 것이 맞다. 그런데 그게 아닌 경우에는 과대적합과 과소적합으로 나뉘어진다.
과대적합: 훈련 세트에서 점수가 좋았는데 테스트 세트에서 점수가 적당히가 아니라 굉장히 나쁜 경우.
과소적합: 훈련 세트보다 테스트 세트의 점수가 높거나 두 점수가 모두 너무 굉장히 낮은 경우.
    과소적합은 모델이 너무 단순하여 훈련 세트에 적절히 훈련되지 않은 경우에 일어난다.

우리의 농어 프로그램은 훈련 세트보다 테스트 세트의 점수가 높은 과소적합을 보인다. 이는 모델이 너무 단순하여 일어나는 문제이므로 좀 더 디테일하고 복잡하게 만들면 된다.
방법론적으로는, 근처에 존재하는 k의 개수를 조금 줄이는 것이다. 이웃의 개수를 줄이면 훈련 세트에 있는 부분적인 패턴에 민감해진다.
k value is 5 by default. thus we do n_neighbors = 3 instead. the result now shows correctly (test model score > train model score)

To sum up,
we used KNN regression model. we put bunch of perch lengths as input and according weights as outputs.
when we do the program regularly we get underfitting. the main problem was that the model is too simple and thus it's not trained enough.
so we put 3 for n_neighbors instead of 5. now there's no more underfitting problem.


● 선형 회귀 Linear Regression

KNN 알고리즘의 한계: 위의 모델에 50cm 짜리 농어를 넣어보니 오류가 뜬다. 우리의 결과값은 1033g인데 실제 농어의 무게는 훨 무겁다.
                     이런 식으로 문제가 생겼을 경우에는 보통 그래프로 시각화를 하면 크게 도움이 된다. 첨부한 링크를 따라가면 코랩 파일에 그래프 및 코드 다 있다.
                     이유인즉슨, 우리가 넣은 훈련 데이터에서 가장 큰 값들은 45cm 언저리밖에 없기 때문에 그 이상이 되는 녀석들을 넣으면 같은 값 리턴.
                     즉, 우리가 가진 데이터로 KNN 알고리즘을 돌렸을 때 부정하거나 수정할 수 없는 불가피한 오류인 것이다. KNN이 아닌 다른 알고리즘을 찾아야 한다.
                     이때 대신 사용하려는 알고리즘 이름이 바로 linear regression 이다.

Linear Regression: 대표적인 회귀 알고리즘. 비교적 간단하고 성능이 뛰어남. 특성이 하나인 경우 직선을 학습하는 알고리즘.
                    처음 머신러닝이라고 했을 때 내가 직관적으로 생각했던 알고리즘과 같다. 기존의 데이터들을 추합해서 직선을 만들고 해당 직선 위에서 답을 찾는 것이다.
                    사이킷런의 sklearn.linear_model 패키지 아래에 LinearRegression 클래스로 선형 회귀 알고리즘이 있으므로 이를 사용하면 된다.
                    
* 머신러닝에서 기울기를 계수 Coefficient 또는 가중치 Weight 라고 부른다. model parameter라고 부르는데, 많은 머신러닝 알고리즘의 훈련 과정은 최적의 모델 파라미터를 찾는 것.
  이게 모델 기반 학습. 앞서 사용한 k-NN 은 모델 파라미터가 없고 훈련 세트를 저장하는 것이 다였는데 이런 것을 사례 기반 학습이라고 부름.

선형 회귀를 이용하여 진행해봤는데, 훈련 점수 0.94에 테스트 점수 0.82로, 둘 사이의 차이가 크지 않으므로 과대적합이 아니고 오히려 둘 다 낮으므로 과소적합으로 보임.
게다가 다른 문제도 있다. 뭐가 문제냐면, 그래프 상 왼쪽 아래는 곡선인데 직선으로 regression 하다보니 제대로 회귀를 할 수 없었던 것. 이때 쓰는 게 다항 회귀.


● 다항 회귀 Polynomial Regression

선형 회귀의 한계점을 보완하는 녀석. 선형은 일차원, 다항은 다차원이니까 사실 그냥 선형의 업그레이드 버전이라고 봐도 무방하다.
기존의 프로그램에서 제곱으로 만든 데이터를 하나 추가해주고 나머지는 그대로 사용해주는 것이다. 재밌는 점은, 분명 polynomial regression인데 linear regression의 기존 식에 넣음.
p 141에 자세히 나와있지만, 어떤 녀석의 제곱 값을 다른 변수로 치환해주면 그게 곧 linear라는 것이다.
구간을 잘게 나누어 점을 찍고 리니어 식에 넣으면 수많은 짧은 직선들이 모여 큰 곡선을 나타낸다. 우리는 훈련 스코어 0.971, 테스트 스코어 0.978 의 점수를 받는데 아직 과소적합.
정확히 말하면 다항은 맞는데 다항 알고리즘을 쓰는 게 아니라 선형 알고리즘에 다항식적 계산을 첨가해주는 것이다. 
그러니까, 다항식적으로 계산을 해주되 우리가 사용하는 식 자체는 선형이기에 그 단위를 잘게 쪼개어 마치 곡선처럼 보이게끔 해주는 것이다. 엄밀히 말하면 다항을 쓰는 게 아닌 것.


● 특성 공학과 규제 Feature Engineering and Regulations

그러나 이차식을 적용해서 푼 위의 문제마저도 테스트 케이스가 훈련 케이스보다 스코어 점수가 높다는 점이 찜찜하다. 이 문제를 해결하려면 모델을 조금 더 디테일하게 만들어서
훈련모델을 충분히 훈련시켜야 한다. 그런데 기존의 이차식도 적용이 쉽지 않았는데 더 고차식을 다루기도 부담스럽고 기술적으로도 계산도 어렵다.
그렇다면 다른 요소를 건드려보는 것은 어떨까? 지금까지는 인풋 데이터를 고정하고 모델을 고차원화 시키는 방식을 사용했는데 그게 아니라 데이터를 추가해서 풍부하게 만드는 것.
다행히도 우리는 위에서 쓴 '길이' 특성 말고도 '높이', '두께' 등의 특성 데이터를 보유하고 있다. 이 친구들을 사용해주면 문제가 해결될까.

● 다중 회귀 Multiple Regression

이제 기하적으로 좀 더 생각을 해줘야 하는 모델이 등장한다. 
일반 선형 회귀는 일차원적 특성 한 가지를, 다항 회귀는 다차원적 특성 한 가지를 썼다면 다중 회귀는 일차원적 특성을 여러가지 사용하는 것이다.
일반 선형 회귀처럼 1개의 특성을 사용했을 때는 직선이었고, 다항 회귀는 특성은 하나지만 결과적으로 나온 모델은 평면상의 이차함수였다면
다항회귀는 n개의 특성이 있다면 n차원 상에 그래프가 생성되는 것이다.

이 방법에서 우린 농어의 길이 말고도 농어의 높이와 두께도 함께 사용한다. Plus, 이전 절처럼 3개의 특성을 각각 제곱하여 추가한다. 게다가 특성을 서로 곱해서 또 다른 특성을 만든다.
예를 들면 농어 길이 * 농어 높이 이렇게. 기존의 특성을 사용해 새 특성을 뽑아내는 작업을 특성 공학 Feature Engineering이라고 부른다.

우선 농어의 특성이 3개로 늘어나서 데이터를 받고 저장하는데 수월하지 않아졌다. 그러나 우리는 수월하게 만든다. pandas라는 데이터 분석 라이브러리.
pandas의 핵심 데이터 구조인 dataframe. 넘파이 배열과 비슷하게 다차원 배열을 다룰 수 있지만 훨 많은 기능을 제공. 또 이를 넘파이 배열로 바꾸는 것도 어렵지 않다.
pandas 에서 많이 사용하는 파일은 CSV 파일. 이는 콤마로 나누어져 있는 텍스트 파일이다.
이 파일은 좋은 점이, 판다스.read_csv()만 해주면 파일을 읽고 저장할 수 있다. 그리고 나서 해당 파일을 .to_numpy() 해주면 자료가 넘파이로 바뀐다.

- 사이킷런의 변환기

사이킷런은 다양한 클래스를 제공하는데, 모델 클래스에 fit(), score(), predict() 메서드가 있는 것처럼 변환기 클래스는 모두 fit(), transform() 메서드를 제공함.
우리는 PolynomialFeatures라는 변환기를 사용할 예정이다. 이 클래스는 sklearn.preprocessing 패키지에 포함되어 있다.

transform 이라는 메소드를 알기만 하면 내부의 매커니즘을 완전히 파악하지 않더라도 알아보는 것이 가능하다. 다만 매커니즘을 이해해야 나중에 편하겠다.
기존의 예제를 2개의 특성을 추가해서 만든 알고리즘은 더 이상 과소적합 문제를 나타내지 않는다. 그럼 알고리즘을 고도화할수록 이득인 부분?
그렇지 않다. 왜냐하면 너무 고도화 되면 훈련 세트에 너무 과대적합되면서 그 두 개가 동일시 되어 버린다는 문제가 발생한다는 것이다.
42개의 훈련 세트 샘플 개수를 가지고 55개짜리 특성을 만든 꼴인 것이다. 이렇게 극단적인 두 예시 속에서 균형잡힌 모델을 만드는 것이 관건이다.


● Regularization

규제는 머신러닝 모델이 훈련 세트를 너무 과도하게 학습하지 못하도록 방지하는 것을 말한다. 즉, 모델이 훈련 세트에 과대적합 되지 않도록 만드는 것.
선형 회귀 모델의 경우 특성에 곱해지는 계수의 크기를 작게 만드는 일.         <-- 자동차로 치면 급커브를 방지하는 것.

일반적으로 선형 회귀 모델에 규제를 적용할 때 계수 값의 크기가 서로 많이 다르면 공정하게 제어되지 않을 것. 규제를 적용하기 전에 정규화를 해야 함.
이전에는 평균과 표준편차를 직접 구해 특성을 표준점수로 바꿈. 이 방법도 쉽지만, 이번엔 사이킷런에서 제공하는 StandardScaler 클래스를 사용.


● 릿지 회귀와 라쏘 회귀: (p 160~ )
선형 모델에 규제를 추가한 모델. 사이킷런 모델을 사용할 때 편리한 점은 훈련 및 사용 방법이 항상 같다는 것.
모델 객체를 만들고 fit() 메서드에서 훈련한 다음 score() 메서드로 평가. 앞서 준비한 train_scaled 데이터로 릿지 모델을 훈련.
** the book needs to be reviewed, especially in chapter 3
