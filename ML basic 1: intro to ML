21.03.30.   [p26 ~ p111]
Chapter 01, 02. intro to ML and dealing with data


● AI, Machine Learning, Deep Learning 인트로

1940, 50: 인공지능 태동기  : Turing Test
1960, 70: 인공지능 황금기  : 다트머스 AI 컨퍼런스
1970:     1차 AI 겨울      : 컴퓨터 성능 한계
1980:     AI 붐            : 전문가 시스템
1990:     2차 AI 겨울      : 전문가 시스템 실패
1998 - :        BOOM!      : LeNet-5, AlexNet, Tensorflow, AlphaGo...

우리가 영화 속에서 보던 인공지능은 Artificial General Intelligence / Strong AI 라고 부르는 인공지능 - 사람과 구분하기 어려울 정도의 지능을 가진 인공지능.
우리가 현실에서 마주하는 인공지능은 Week AI - 특정 분야에서 사람의 일을 도와주는 보조 역할.


● Machine Learning?

AI의 분야 중 규칙을 일일이 프로그래밍하지 않아도 자동으로 데이터에서 규칙을 학습하는 알고리즘을 연구하는 분야. 대표적인 CS 머신러닝 라이브러리: Scikit-Learn.
Scikit-Learn과 같은 라이브러리의 구조: 연구자들이 새로운 알고리즘을 끊임없이 개발, 발표 -> 대중들의 검증 -> 사이킷런 라이브러리 개발자들의 검증 및 추가.
즉, 기존 라이브러리의 알고리즘이 검증되어 있으므로 프로그래머가 직접 모든 알고리즘을 구현하고 프로그래밍할 필요가 없다.


● Deep Learning?

많은 ML 알고리즘 중에 Artificial Neural Network 를 기반으로 한 방법들을 통칭하여 Deep Learning 이라고 부른다. 인공신경망과 딥러닝을 크게 구분하지 않기도 한다.
두 번째 AI 겨울 기간에도 여전히 인공지능에 대해 연구한 사람들이 있었는데, 1998년 신경망 모델을 만들어 LeNet-5 이라는 합성곱 신경망을 성공적으로 구현함.
2012년 Geoffrey Hinton의 팀이 ImageNet에서 AlexNet으로 압도적 성능으로 우승. 이 역시 합성곱 신경망을 사용함.
지금 또 다시 겨울이 오지 않는 큰 이유: 오픈 소스 라이브러리의 영향력. 구글의 Tensorflow, 페이스북의 PyTorch 의 딥러닝 오픈소스 라이브러리를 기반으로 성장 중.


● Colab & Jupyter Notebook

Colab은 구글이 대화식 프로그래밍 환경인 주피터를 커스터마이징한 개발 환경. 웹 브라우저에서 파이썬을 테스트하고 저장할 수 있는 서비스. 머신러닝 프로그램도 만들 수 있음.
        장점은 클라우드를 사용하므로 컴퓨터 사양과 상관없이 프로그램을 실습해 볼 수 있다는 점.
        Colab은 구글 클라우드의 VM을 사용하므로 한정된 12G 메모리와 100G 디스크를 제공받는다. 최대 5개의 서버(노트북)를 동시에 열 수 있고 각 서버는 12시간까지 실행 가능.
        
        
● 마켓과 머신러닝 : K - Nearest Neighbors (KNN) 알고리즘

'도미'라는 생선을 다양한 생선군들 사이에서 찾아낼 능력을 기르는 머신러닝. 간단한 것부터 해결해야 하므로 우선 도미와 빙어를 구분하는 것으로 시작.
35마리의 도미의 무게와 길이를 두 개의 리스트에 각각 나누어 둠. 무게와 길이는 '특성'이라고 부른다.
시각화를 위해서 scatter plot(산점도)을 사용한다. 파이썬에서 과학계산용 그래프를 그리는 대표적인 패키지는 matplotlib, 그 안에 pyplot. 그 중 scatter() 함수를 사용.

모든 코딩 실습은 코랩으로 진행. 앞으로도 이런 식으로 링크를 첨부할 예정.
https://colab.research.google.com/drive/1fahkIRp_mohBJhAaX5BjK29Wa6jmeoVI#scrollTo=ki4U5cf48dyW

실습한 과정에서 가장 중요하다고 판단되는 녀석들을 짚고 넘어가자면,
우선 사이킷런은 2차원 리스트를 필요로 하므로 주어진 자료들을 2차원 리스트로 변형시키는 것이 필요. 이 과정에서 zip 함수 사용.
변형 후에는 사이킷런 모듈에서 내가 필요로 하는 알고리즘(이번 경우에는 KNN 알고리즘).           from sklearn.neighbors import KNeighborsClassifier
해당 클래스의 객체를 먼저 생성한다.                                                            kn = KNeighborsClassifier()
해당 객체를 준비된 데이터를 주며 훈련시킨다.                                                   kn.fit(fish_data, fish_target)
훈련시킨 녀석을 정확도를 토대로 '점수화'한다.                                                  kn.score(fish_data, fish_target)

위의 과정에서 사용됨 K-최근접 이웃 알고리즘은 상당히 간단함. 어떤 데이터에 대한 답을 구할 때 주위의 다른 데이터를 보고 다수를 차지하는 것을 정답으로 예측한다.

* 사실 위의 KNN 알고리즘은 뭔가 훈련되는 게 없는 셈이다. 그냥 fit() 메서드에 전달한 데이터를 모두 저장하고 있다가 새 데이터가 나오면 그걸 기존 데이터를 토대로 '계산'하는 것 뿐.

기본적으로 KNN에서는 가까운 5개의 데이터를 참고하여 답을 도출한다. 즉 가장 가까운 5개의 녀석들을 기준으로 다수결로 판단하는 것이다.
이 개수를 30개로 늘리고 싶다면 그렇게 커스터마이즈 해주면 된다.                                 kn30 = KNeighborsClassifier(n_neighbors=30)


● 훈련 세트와 테스트 세트

머신러닝 알고리즘은 지도 학습과 비지도 학습(supervised learning and unsupervised learning) 으로 나뉜다. 지도학습은 알고리즘을 훈련하기 위한 데이터와 정답이 필요.
지도 학습에서는 데이터와 정답을 input과 target이라 하고, 이 둘을 합쳐 훈련 데이터(training data) 라고 부른다. 입력으로 사용된 길이와 무게는 특성(feature)이라고 부른다.
비지도 학습에서는 input만 있을 뿐, target이 없다. 이런 종류에서는 답을 도출할 수가 없다는데, 이는 나중에 챕터 6에서 디테일하게 공부하자.

본론으로 돌아와서, 위의 market 예제에서는 정규시험에 모의고사로 훈련한 내용이 그대로 나온 꼴이 되었다. 이러니 기억력에 거의 한계가 없는 컴퓨터가 만점 못 받으면 이상한 것이다.
즉 우리가 만든 프로그램의 성능을 테스트하려면 우리가 훈련한 훈련 세트와는 다른 세트, 즉 새로운 테스트 세트로 테스트를 해야 하는 것이다.

알고리즘을 만드는 것은, 해당 알고리즘을 짜고 샘플링하고 오차가 없게 만드는 계획과 알고리즘을 구현하는 기술적인 실행으로 나뉘는데 '실행' 파트는 기계적으로 배우면 되지만
                        '계획' 파트에서 문제가 없도록(대표적으로 '샘플링 편향') 하는 것이 중요하다. 이 부분은 컴퓨터에서 알려주지도 않기 때문이다.


● NumPy

stands for Numerical Python. 대표적인 배열 라이브러리. 기존에 애용하던 list로 2D까지는 표현이 어렵지 않으나 고차원 리스트를 표현하려면 매우 번거로움.
numpy는 고차원의 배열을 손쉽게 만들고 조작할 수 있는 간편한 도구를 제공함. 이제 다시 생선 데이터로 돌아가서 NumPy 실무적용연습을 시작한다.

이번에는 기존의 데이터 셋에서 랜덤하게 샘플을 선택해서 훈련 세트 및 테스트 세트를 만들 차례. 당연하지만, 각각의 세트의 인덱스를 맞춰줘야만 한다는 점 유의! (p76 읽으면 더 수월)
그러니 각각의 세트를 건드리기 보다는 공통으로 해당되는 녀석 - 바로 인덱스! - 을 랜덤으로 섞고 거기서 필요한 몇 개만 가져오는 것.

도중에 arange()를 쓰던 중 왜 range()를 쓰는가 하는 의문이 들었다. 
둘은 비슷하지만 확실히 다른 부분이 있다. arange()는 numpy에 특화된 녀석이다. 속도도 훨 빠르고, range는 int만을 받지만 arange는 다 받는다. 
random 패키지 아래에 있는 shuffle() 함수는 주어진 배열을 무작위로 섞는다.

to sum up, 처음에 일치율 100% 짜리 모델을 만들었지만 알고리즘이 제대로 작동하는지를 확인하려면 훈련한 데이터 세트와 결과를 확인할 때 쓰는 데이터 세트가 서로 달라야 한다.
그래서 새로운 데이터로 새로운 세트를 만들어주었다. 여기서 훈련하는 데이터와 결과값을 확인하는 데이터를 나눌 때, 샘플링 편향이 일어나지 않도록 하기 위해 random의 shuffle()을 사용했다.
-> 물론 이번 경우에는 총 표본의 수가 49개 밖에 되지 않기에 그냥 진행해주었지만, shuffle()이 얼마나 디테일한 부분까지 편향을 막아줄지는 모르겠다. 일단은 계속 진행.


● 데이터 전처리 Data Preprocessing

'수상한 도미 한 마리' 케이스에서 25, 150의 수를 넣었을 때 분명히 도미여야 마땅한 수인데 함수값은 계속 0, 그러니까 빙어를 반환했다.
알고리즘 상의 한계 - 그러니까 특이 케이스를 솎아내지 못하는 것 - 인줄 알았는데 그게 아니었다. 제대로 된 KNN 함수라면 1을 반환해야 하는 케이스인 것이다.
그렇다면 무슨 문제인 것인가? 바로 단위의 문제였다. 즉, 25, 150의 숫자에서도 알 수 있지만 단위 자체가 길이보다는 무게가 몇 배 더 크게 간다.
우리의 프로그램은 이 스케일을 그대로 갖다쓴 나머지 길이보다는 전반적인 크기가 큰 무게에 좀 더 편중된 값을 선보였던 것이다.
알고리즘이 거리를 기반으로 할 때 흔히 나타날 수 있는 문제점이다. 그래서 샘플 간의 거리에 영향을 덜 받고 정확한 값을 얻기 위해 데이터를 일정 기준으로 맞추는 걸 "데이터 전처리"라고 .

가장 널리 사용하는 전처리 방법 중 하나는 표준점수 Standard score, 혹은 Z 점수라고 부름. 각 특성값이 0에서 표준편차의 몇 배만큼 떨어져 있는지를 나타냄. 실제 특성값의 크기 무력화.
공식은, 평균을 빼고 표준편차를 나눠주면 된다.

mean = np.mean(train_input, axis=0)             <- 평균
std = np.std(train_input, axis=0)               <- 표준편차
train_scaled = (train_input - mean) / std       <- 이것만 해주면 넘파이는 알아서 train_input의 모든 행에서 이걸 적용해줌. 이 매커니즘은 Broadcasting이라고 불림.

이렇게 해당 함수를 깔끔하게 스케일링해주고 나서 문제의 도미도 같은 방법으로 scaling하고 나면 print(kn.predict([new])) 가 드디어 옳은 값인 1을 리턴한다.


● 배열을 다루면서 list와 numpy의 달랐던 점

1 사이킷런의 인풋 꼴을 맞추기 위해서는
list:           fish_data = [[l, w] for l, w in zip(length, weight)]
NumPy:          fish_data = np.column_stack((fish_length, fish_weight))

2 타겟을 만들 때 특정한 개수의 1과 0으로 채울 때는
list:           fish_target = [1] * 35  + [0] * 14
NumPy:          fish_target = np.concatenate((np.ones(35), np.zeros(14)))

* 1의 NumPy에서 보이지만 column_stack(())의 경우 더블 괄호를 필요로 한다.

<챕터 1, 2 >
