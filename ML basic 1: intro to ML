21.03.30.   [p26 ~ p]
Chapter 01, 02. intro to ML and dealing with data


● AI, Machine Learning, Deep Learning 인트로

1940, 50: 인공지능 태동기  : Turing Test
1960, 70: 인공지능 황금기  : 다트머스 AI 컨퍼런스
1970:     1차 AI 겨울      : 컴퓨터 성능 한계
1980:     AI 붐            : 전문가 시스템
1990:     2차 AI 겨울      : 전문가 시스템 실패
1998 - :        BOOM!      : LeNet-5, AlexNet, Tensorflow, AlphaGo...

우리가 영화 속에서 보던 인공지능은 Artificial General Intelligence / Strong AI 라고 부르는 인공지능 - 사람과 구분하기 어려울 정도의 지능을 가진 인공지능.
우리가 현실에서 마주하는 인공지능은 Week AI - 특정 분야에서 사람의 일을 도와주는 보조 역할.


● Machine Learning?

AI의 분야 중 규칙을 일일이 프로그래밍하지 않아도 자동으로 데이터에서 규칙을 학습하는 알고리즘을 연구하는 분야. 대표적인 CS 머신러닝 라이브러리: Scikit-Learn.
Scikit-Learn과 같은 라이브러리의 구조: 연구자들이 새로운 알고리즘을 끊임없이 개발, 발표 -> 대중들의 검증 -> 사이킷런 라이브러리 개발자들의 검증 및 추가.
즉, 기존 라이브러리의 알고리즘이 검증되어 있으므로 프로그래머가 직접 모든 알고리즘을 구현하고 프로그래밍할 필요가 없다.


● Deep Learning?

많은 ML 알고리즘 중에 Artificial Neural Network 를 기반으로 한 방법들을 통칭하여 Deep Learning 이라고 부른다. 인공신경망과 딥러닝을 크게 구분하지 않기도 한다.
두 번째 AI 겨울 기간에도 여전히 인공지능에 대해 연구한 사람들이 있었는데, 1998년 신경망 모델을 만들어 LeNet-5 이라는 합성곱 신경망을 성공적으로 구현함.
2012년 Geoffrey Hinton의 팀이 ImageNet에서 AlexNet으로 압도적 성능으로 우승. 이 역시 합성곱 신경망을 사용함.
지금 또 다시 겨울이 오지 않는 큰 이유: 오픈 소스 라이브러리의 영향력. 구글의 Tensorflow, 페이스북의 PyTorch 의 딥러닝 오픈소스 라이브러리를 기반으로 성장 중.


● Colab & Jupyter Notebook

Colab은 구글이 대화식 프로그래밍 환경인 주피터를 커스터마이징한 개발 환경. 웹 브라우저에서 파이썬을 테스트하고 저장할 수 있는 서비스. 머신러닝 프로그램도 만들 수 있음.
        장점은 클라우드를 사용하므로 컴퓨터 사양과 상관없이 프로그램을 실습해 볼 수 있다는 점.
        Colab은 구글 클라우드의 VM을 사용하므로 한정된 12G 메모리와 100G 디스크를 제공받는다. 최대 5개의 서버(노트북)를 동시에 열 수 있고 각 서버는 12시간까지 실행 가능.
        
        
● 마켓과 머신러닝 : K - Nearest Neighbors (KNN) 알고리즘

'도미'라는 생선을 다양한 생선군들 사이에서 찾아낼 능력을 기르는 머신러닝. 간단한 것부터 해결해야 하므로 우선 도미와 빙어를 구분하는 것으로 시작.
35마리의 도미의 무게와 길이를 두 개의 리스트에 각각 나누어 둠. 무게와 길이는 '특성'이라고 부른다.
시각화를 위해서 scatter plot(산점도)을 사용한다. 파이썬에서 과학계산용 그래프를 그리는 대표적인 패키지는 matplotlib, 그 안에 pyplot. 그 중 scatter() 함수를 사용.

모든 코딩 실습은 코랩으로 진행. 앞으로도 이런 식으로 링크를 첨부할 예정.
https://colab.research.google.com/drive/1fahkIRp_mohBJhAaX5BjK29Wa6jmeoVI#scrollTo=ki4U5cf48dyW

실습한 과정에서 가장 중요하다고 판단되는 녀석들을 짚고 넘어가자면,
우선 사이킷런은 2차원 리스트를 필요로 하므로 주어진 자료들을 2차원 리스트로 변형시키는 것이 필요. 이 과정에서 zip 함수 사용.
변형 후에는 사이킷런 모듈에서 내가 필요로 하는 알고리즘(이번 경우에는 K-NN 알고리즘).           from sklearn.neighbors import KNeighborsClassifier
해당 클래스의 객체를 먼저 생성한다.                                                            kn = KNeighborsClassifier()
해당 객체를 준비된 데이터를 주며 훈련시킨다.                                                   kn.fit(fish_data, fish_target)
훈련시킨 녀석을 정확도를 토대로 '점수화'한다.                                                  kn.score(fish_data, fish_target)

위의 과정에서 사용됨 K-최근접 이웃 알고리즘은 상당히 간단함. 어떤 데이터에 대한 답을 구할 때 주위의 다른 데이터를 보고 다수를 차지하는 것을 정답으로 예측한다.

* 사실 위의 KNN 알고리즘은 뭔가 훈련되는 게 없는 셈이다. 그냥 fit() 메서드에 전달한 데이터를 모두 저장하고 있다가 새 데이터가 나오면 그걸 기존 데이터를 토대로 '계산'하는 것 뿐.

기본적으로 KNN에서는 가까운 5개의 데이터를 참고하여 답을 도출한다. 즉 가장 가까운 5개의 녀석들을 기준으로 다수결로 판단하는 것이다.
이 개수를 30개로 늘리고 싶다면 그렇게 커스터마이즈 해주면 된다.                                 kn30 = KNeighborsClassifier(n_neighbors=30)


● 훈련 세트와 테스트 세트

머신러닝 알고리즘은 지도 학습과 비지도 학습(supervised learning and unsupervised learning) 으로 나뉜다. 지도학습은 알고리즘을 훈련하기 위한 데이터와 정답이 필요.
지도 학습에서는 데이터와 정답을 input과 target이라 하고, 이 둘을 합쳐 훈련 데이터(training data) 라고 부른다. 입력으로 사용된 길이와 무게는 특성(feature)이라고 부른다.
비지도 학습에서는 input만 있을 뿐, target이 없다. 이런 종류에서는 답을 도출할 수가 없다는데, 이는 나중에 챕터 6에서 디테일하게 공부하자.

본론으로 돌아와서, 위의 market 예제에서는 정규시험에 모의고사로 훈련한 내용이 그대로 나온 꼴이 되었다. 이러니 기억력에 거의 한계가 없는 컴퓨터가 만점 못 받으면 이상한 것이다.
즉 우리가 만든 프로그램의 성능을 테스트하려면 우리가 훈련한 훈련 세트와는 다른 세트, 즉 새로운 테스트 세트로 테스트를 해야 하는 것이다.

알고리즘을 만드는 것은, 해당 알고리즘을 짜고 샘플링하고 오차가 없게 만드는 계획과 알고리즘을 구현하는 기술적인 실행으로 나뉘는데 '실행' 파트는 기계적으로 배우면 되지만
                        '계획' 파트에서 문제가 없도록 하는 것이 중요하다. 이 부분은 컴퓨터에서 알려주지도 않기 때문이다.


● NumPy
stands for Numerical Python. 대표적인 배열 라이브러리. 기존에 애용하던 list로 2D까지는 표현이 어렵지 않으나 고차원 리스트를 표현하려면 매우 번거로움.
numpy는 고차원의 배열을 손쉽게 만들고 조작할 수 있는 간편한 도구를 제공함. 이제 다시 생선 데이터로 돌아가서 NumPy 실무적용연습을 시작한다.

이번에는 기존의 데이터 셋에서 랜덤하게 샘플을 선택해서 훈련 세트 및 테스트 세트를 만들 차례. 당연하지만, 각각의 세트의 인덱스를 맞춰줘야만 한다는 점 유의! (p76 읽으면 더 수월)
그러니 각각의 세트를 건드리기 보다는 공통으로 해당되는 녀석 - 바로 인덱스! - 을 랜덤으로 섞고 거기서 필요한 몇 개만 가져오는 것.
